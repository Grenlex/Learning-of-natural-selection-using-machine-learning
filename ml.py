# -*- coding: utf-8 -*-
"""ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ErQb3lsBdGyfv4sZgK587N2WZzSxBeKy

# Simple Feedforward Network
"""

import numpy as np
 
 
def sigmoid(x):
    # Функция активации sigmoid:: f(x) = 1 / (1 + e^(-x))
    return 1 / (1 + np.exp(-x))
 
 
def deriv_sigmoid(x):
    # Производная от sigmoid: f'(x) = f(x) * (1 - f(x))
    fx = sigmoid(x)
    return fx * (1 - fx)
 
 
def mse_loss(y_true, y_pred):
    # y_true и y_pred являются массивами numpy с одинаковой длиной
    return ((y_true - y_pred) ** 2).mean()
 
 
class OurNeuralNetwork:
    """
    Нейронная сеть, у которой:
        - 2 входа
        - скрытый слой с двумя нейронами (h1, h2)
        - слой вывода с одним нейроном (o1)
 
    *** ВАЖНО ***:
    Код ниже написан как простой, образовательный. НЕ оптимальный.
    Настоящий код нейронной сети выглядит не так. НЕ ИСПОЛЬЗУЙТЕ этот код.
    Вместо этого, прочитайте/запустите его, чтобы понять, как работает эта сеть.
    """
    def __init__(self):
        # Вес
        self.w1 = np.random.normal()
        self.w2 = np.random.normal()
        self.w3 = np.random.normal()
        self.w4 = np.random.normal()
        self.w5 = np.random.normal()
        self.w6 = np.random.normal()
 
        # Смещения
        self.b1 = np.random.normal()
        self.b2 = np.random.normal()
        self.b3 = np.random.normal()
 
    def feedforward(self, x):
        # x является массивом numpy с двумя элементами
        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)
        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)
        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)
        return o1
 
    def train(self, data, all_y_trues):
        """
        - data is a (n x 2) numpy array, n = # of samples in the dataset.
        - all_y_trues is a numpy array with n elements.
            Elements in all_y_trues correspond to those in data.
        """
        learn_rate = 0.1
        epochs = 1000 # количество циклов во всём наборе данных
 
        for epoch in range(epochs):
            for x, y_true in zip(data, all_y_trues):
                # --- Выполняем обратную связь (нам понадобятся эти значения в дальнейшем)
                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1
                h1 = sigmoid(sum_h1)
 
                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2
                h2 = sigmoid(sum_h2)
 
                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3
                o1 = sigmoid(sum_o1)
                y_pred = o1
 
                # --- Подсчет частных производных
                # --- Наименование: d_L_d_w1 представляет "частично L / частично w1"
                d_L_d_ypred = -2 * (y_true - y_pred)
 
                # Нейрон o1
                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)
                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)
                d_ypred_d_b3 = deriv_sigmoid(sum_o1)
 
                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)
                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)
 
                # Нейрон h1
                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)
                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)
                d_h1_d_b1 = deriv_sigmoid(sum_h1)
 
                # Нейрон h2
                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)
                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)
                d_h2_d_b2 = deriv_sigmoid(sum_h2)
 
                # --- Обновляем вес и смещения
                # Нейрон h1
                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1
                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2
                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1
 
                # Нейрон h2
                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3
                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4
                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2
 
                # Нейрон o1
                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5
                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6
                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3
 
            # --- Подсчитываем общую потерю в конце каждой фазы
            if epoch % 10 == 0:
                y_preds = np.apply_along_axis(self.feedforward, 1, data)
                loss = mse_loss(all_y_trues, y_preds)
                print("Epoch %d loss: %.3f" % (epoch, loss))
 
 
# Определение набора данных
data = np.array([
    [-2, -1],    # Alice
    [25, 6],     # Bob
    [17, 4],     # Charlie
    [-15, -6], # Diana
])
 
all_y_trues = np.array([
    1, # Alice
    0, # Bob
    0, # Charlie
    1, # Diana
])
 
# Тренируем нашу нейронную сеть!
network = OurNeuralNetwork()
network.train(data, all_y_trues)

"""# RNN"""

import numpy as np
import random

from rnn import RNN
from data import train_data, test_data

# Create the vocabulary.
vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))
vocab_size = len(vocab)
print('%d unique words found' % vocab_size)

# Assign indices to each word.
word_to_idx = { w: i for i, w in enumerate(vocab) }
idx_to_word = { i: w for i, w in enumerate(vocab) }
# print(word_to_idx['good'])
# print(idx_to_word[0])

def createInputs(text):
  '''
  Returns an array of one-hot vectors representing the words in the input text string.
  - text is a string
  - Each one-hot vector has shape (vocab_size, 1)
  '''
  inputs = []
  for w in text.split(' '):
    v = np.zeros((vocab_size, 1))
    v[word_to_idx[w]] = 1
    inputs.append(v)
  return inputs

def softmax(xs):
  # Applies the Softmax Function to the input array.
  return np.exp(xs) / sum(np.exp(xs))

# Initialize our RNN!
rnn = RNN(vocab_size, 2)

def processData(data, backprop=True):
  '''
  Returns the RNN's loss and accuracy for the given data.
  - data is a dictionary mapping text to True or False.
  - backprop determines if the backward phase should be run.
  '''
  items = list(data.items())
  random.shuffle(items)

  loss = 0
  num_correct = 0

  for x, y in items:
    inputs = createInputs(x)
    target = int(y)

    # Forward
    out, _ = rnn.forward(inputs)
    probs = softmax(out)

    # Calculate loss / accuracy
    loss -= np.log(probs[target])
    num_correct += int(np.argmax(probs) == target)

    if backprop:
      # Build dL/dy
      d_L_d_y = probs
      d_L_d_y[target] -= 1

      # Backward
      rnn.backprop(d_L_d_y)

  return loss / len(data), num_correct / len(data)

# Training loop
for epoch in range(1000):
  train_loss, train_acc = processData(train_data)

  if epoch % 100 == 99:
    print('--- Epoch %d' % (epoch + 1))
    print('Train:\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))

    test_loss, test_acc = processData(test_data, backprop=False)
    print('Test:\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))

import numpy as np
import random
#f=open('galton.txt', 'r')

train_data={
'5.6 4.5 7.8 5.7 3.4 4.9 6.2 4.8 5.1 5.5': 'october',
'19.2 22.7 17.7 16.5 25.1 23.8 19.9 18.1 23.3 17.9': 'july',
'5.8 4.9 3.3 5.5 5.0 7.8 6.2 6.1 4.4 4.9': 'october',
'25.4 26.2 19.2 18.5 16.3 18.1 21.9 20.1 22.7 19.5' : 'july',
}

test_data = {
    '7.2 8.4 5.3 5.6 4.8 4.9 5.9 4.4 5.1 5.8' : 'october',
    #'-1000 -1000 -1000 -1000 -1000 -1000 -1000 -1000 -1000 -1000' : 'july',
}

class RNN:
  
  def __init__(self, input_size, output_size, hidden_size=32):
    # конструктор класса, создаём матрицы коэффициентов
    self.Whh = np.random.randn()
    self.Wxh = np.random.randn()
    self.Why = np.random.randn(output_size, 1)

    # задаём смещения
    self.bh = 0
    self.by = 0

  def forward(self, inputs): # пишем фазу прямого распространения
    
    h = 0 # массив из ноликов

    self.last_inputs = inputs # сохраняем исходный данные для BPTT
    self.last_hs = { 0: h } # заносим исходные данные в словарь

    # пошла фаза прямого распространения
    for i, x in enumerate(inputs):
      h = np.tanh(self.Wxh * float(x) + self.Whh * h + self.bh)
      self.last_hs[i + 1] = h

    # считаем выход
    y = self.Why * h + self.by

    return y, h

  def backprop(self, d_y, learn_rate=2e-2): # функция для обратного распространения
    
    n = len(self.last_inputs)

    # считаем частные производные dL/dWhy и dL/dby.
    d_Why = d_y * self.last_hs[n]
    d_by = d_y

    # деаем dL/dWhh, dL/dWxh, и dL/dbh нулями
    d_Whh = 0
    d_Wxh = 0
    d_bh = 0

    # считаем dL/dh для последнего h.
    # dL/dh = dL/dy * dy/dh
    d_h = self.Why * d_y

    # BPTT
    for t in reversed(range(n)):
      # An intermediate value: dL/dh * (1 - h^2)
      temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)

      # dL/db = dL/dh * (1 - h^2)
      d_bh += temp

      # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}
      d_Whh += temp * self.last_hs[t]

      # dL/dWxh = dL/dh * (1 - h^2) * x
      d_Wxh += temp * float(self.last_inputs[t])

      # Next dL/dh = dL/dh * (1 - h^2) * Whh
      d_h = self.Whh * temp

    # Clip to prevent exploding gradients.
    for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:
      np.clip(d, -1, 1, out=d)

    # Update weights and biases using gradient descent.
    self.Whh -= learn_rate * d_Whh
    self.Wxh -= learn_rate * d_Wxh
    self.Why -= learn_rate * d_Why
    self.bh -= learn_rate * d_bh
    self.by -= learn_rate * d_by




def softmax(xs):
  # считаем Softmax функцию
  return np.exp(xs) / sum(np.exp(xs))



def createInputs(text):
 
  inputs = text.split(' ')
  return inputs


def processData(data, backprop=True):
  
  items = list(data.items())
  random.shuffle(items)

  loss = 0
  num_correct = 0

  for x, y in items:
    
    inputs = createInputs(x)
    if y == 'october':
      target=0
    if y == 'july':
      target=1

    
    # фаза прямого распространения
    out, _ = RNN.forward(rnn, inputs)
    probs = softmax(out)

    

    if backprop:

      # Calculate loss / accuracy
      loss -= np.log(probs[target])
      num_correct += int(np.argmax(probs) == target)

      # Build dL/dy
      d_L_d_y = probs
      d_L_d_y[target] -= 1

      # Backward
      RNN.backprop(rnn, d_L_d_y)

  

    if not backprop:
      if probs[0] > probs[1]:
        print('This is {} with a {} probability'.format('october', max(probs)[0]))
      if probs[0] < probs[1]:
        print('This is {} with a {} probability'.format('july', max(probs)[0]))

  return loss / len(data), num_correct / len(data)


rnn = RNN(10, 2)

for epoch in range(1000):
  train_loss, train_acc = processData(train_data)

  if epoch % 100 == 99:
    print('--- Epoch %d' % (epoch + 1))
    print('Train:\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))

processData(test_data, backprop=False)

"""# Useful commands"""

from google.colab import files 
uploaded = files.upload()

#from rnn import RNN
#from data import train_data, test_data
import numpy as np
from numpy.random import randn
#vocab = list(set([a for text in train_data.keys() for a in text.split(' ')]))
#vocab_size = len(vocab)


#print('%d unique words found' % vocab_size)
#test = set(train_data.keys())
print(a)